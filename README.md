# LLM_instruct_eval
- 专注于大模型对于中文复杂指令的理解能力评估。
- 当前的榜单例如[SuperClue](https://www.cluebenchmarks.com/superclue.html)、[C-Eval](https://yaofu.notion.site/C-Eval-6b79edd91b454e3d8ea41c59ea2af873#6415e97403e34ba6b8b18ac6ef3ff7fc)依然通过做客观选择题的方式评估llm的中文知识能力。我们希望推出一个评估集来全面考量模型对于复杂指令的理解和遵循效果，这类考察更多在于评估llm对人类指令的语言学理解和执行，而不一定要海量的常识或者专业知识。

# 数据集
- 见sample.csv

# 评测分类
- TBD

# Collborators
<a href="https://github.com/HqWu-HITCS"><img src="https://avatars.githubusercontent.com/u/29895268?v=4" alt="图片描述" style="width:5%;"/></a>
<a href="https://github.com/yupeijei1997"><img src="https://avatars.githubusercontent.com/u/39047479?v=4" alt="图片描述" style="width:5%;"/></a>
<a href="https://github.com/thinkwee"><img src="https://avatars.githubusercontent.com/u/11889052?v=4" alt="图片描述" style="width:5%;"/></a>
